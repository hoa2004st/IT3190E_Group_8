{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10173311,"sourceType":"datasetVersion","datasetId":6283365},{"sourceId":195376,"sourceType":"modelInstanceVersion","modelInstanceId":166593,"modelId":188914},{"sourceId":196256,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":167343,"modelId":189661}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport torch\nimport cv2\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:31.324111Z","iopub.execute_input":"2024-12-12T15:33:31.324453Z","iopub.status.idle":"2024-12-12T15:33:32.965518Z","shell.execute_reply.started":"2024-12-12T15:33:31.324422Z","shell.execute_reply":"2024-12-12T15:33:32.964594Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"NO_OF_ANCHOR_BOX = N = 5\nS = 13\nNO_OF_CLASS = C =  1\nHEIGHT = H = 416\nWIDTH = W = 416\nSCALE = 32\n\nDEVICE =device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = batch_size = 32\n\n\nANCHOR_BOXES = A = [[1, 1],\n [ 1,  1.1],\n [1, 1.4],\n [1, 1.3],\n [1, 1.2]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:32.967381Z","iopub.execute_input":"2024-12-12T15:33:32.967891Z","iopub.status.idle":"2024-12-12T15:33:33.014151Z","shell.execute_reply.started":"2024-12-12T15:33:32.967857Z","shell.execute_reply":"2024-12-12T15:33:33.013069Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def convert_to_corners(boxes):\n\n    x_center, y_center, width, height = boxes.unbind(1)\n    x_min = x_center - width / 2\n    y_min = y_center - height / 2\n    x_max = x_center + width / 2\n    y_max = y_center + height / 2\n    return torch.stack((x_min, y_min, x_max, y_max), dim=1)\n\ndef match_anchor_box(bbox_w, bbox_h, to_exclude = [], anchor_boxes =ANCHOR_BOXES):\n    iou = []\n    for i, box in enumerate(anchor_boxes):\n        if i in to_exclude:\n            iou.append(0)\n            continue\n        intersection_width = min(box[0], bbox_w)  # Scale up as h, w in range 0-13\n        intersection_height = min(box[1], bbox_h)  \n        I = intersection_width * intersection_height\n        IOU = I / (bbox_w * bbox_h + box[0] * box[1] - I)\n        iou.append(IOU)\n    \n    iou = torch.tensor(iou)\n    return torch.argmax(iou, dim = 0).item()\ndef intersection_over_union(bb1, bb2):\n    bb1 = bb1.to(DEVICE)\n    bb2 = bb2.to(DEVICE)\n    bboxes = torch.vstack((bb1, bb2))\n    # Convert center-width-height format to top-left and bottom-right format\n    bboxes = convert_to_corners(bboxes)\n    bb1_x1, bb1_y1, bb1_x2, bb1_y2 = bboxes[0]\n    bb2_x1, bb2_y1, bb2_x2, bb2_y2 = bboxes[1]\n    if  bb1_x1 > bb1_x2 or bb1_y1 > bb1_y2 or bb2_x1 > bb2_x2 or bb2_y1 > bb2_y2:\n        return 0\n    x_left = max(bb1_x1, bb2_x1)\n    y_top = max(bb1_y1, bb2_y1)\n    x_right = min(bb1_x2, bb2_x2)\n    y_bottom = min(bb1_y2, bb2_y2)\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    bb1_area = (bb1_x2 - bb1_x1) * (bb1_y2 - bb1_y1)\n    bb2_area = (bb2_x2 - bb2_x1) * (bb2_y2 - bb2_y1)\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n\n    return iou\n           ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:33.015465Z","iopub.execute_input":"2024-12-12T15:33:33.015840Z","iopub.status.idle":"2024-12-12T15:33:33.029615Z","shell.execute_reply.started":"2024-12-12T15:33:33.015812Z","shell.execute_reply":"2024-12-12T15:33:33.028752Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from torchvision.transforms import v2\nfrom torchvision.io import read_image\nfrom torchvision import tv_tensors\n\nclass GDITAerialDataset(Dataset):\n    def __init__(self, rootdir, transform = None):\n        self.root_dir = rootdir\n        self.image_paths = []\n        self.transform = transform\n        \n        class_names = os.listdir(self.root_dir)\n        for directory in class_names:\n            files = os.listdir(os.path.join(self.root_dir, directory))\n    \n            self.image_paths+= [os.path.join(directory, file) for file in files if os.path.splitext(file)[1] =='.jpg']\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        \n        sample = self._make_sample(idx)\n        img, labels, bboxes = sample['image'], sample['labels'],sample['bbox']\n        _, height, width, = img.size()\n\n        target = self._make_target(bboxes, labels, width, height)        \n        \n        return img, target\n\n    def _make_sample(self, idx):\n        img_path = os.path.join(self.root_dir, self.image_paths[idx])\n        target_path = os.path.splitext(img_path)[0]+'.txt'\n        img = read_image(img_path)\n        _, height, width, = img.size()\n        bbox = []\n        labels = [] \n    \n        with open(target_path, 'r') as f:\n            data = f.readlines()\n            for line in data:\n                values = line.split()\n                labels.append(int(values[0]))\n                \n                temp_bbox = [float(val) for val in values[1:]]\n                \n                x, y = temp_bbox[0] * width, temp_bbox[1] * height  # center of the bounding box\n                box_width, box_height = temp_bbox[2] * width, temp_bbox[3] * height\n                bbox+=[[x,y,box_width,box_height]]\n\n        bboxes = tv_tensors.BoundingBoxes(bbox, format=\"CXCYWH\", canvas_size=img.shape[-2:])\n        \n        sample = {\n            'image':img,\n            'labels': torch.tensor(labels),\n            'bbox': bboxes\n        }\n        \n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        return sample\n\n    def _make_target(self, bboxes, labels, height=H, width=W):\n        to_exclude = []\n        target = torch.zeros(S, S, NO_OF_ANCHOR_BOX, 1+4+C )\n        for bbox, label in zip(bboxes, labels):\n            cx, cy = bbox[0]/SCALE, bbox[1]/SCALE \n            pos = (int(cx), int(cy))\n            pos = min(pos[0], 12), min(pos[1], 12)\n            \n            bx, by = cx - int(cx), cy - int(cy)\n            box_widht, box_height = bbox[2]/ SCALE, bbox[3]/SCALE\n            assigned_anchor_box = match_anchor_box(box_widht, box_height, to_exclude)\n            anchor_box = ANCHOR_BOXES[assigned_anchor_box] \n            \n            bw_by_Pw, bh_by_ph = box_widht/anchor_box[0], box_height/anchor_box[1]\n            target[pos[0], pos[1],assigned_anchor_box, 0:5] = torch.tensor([1, bx, by, bw_by_Pw, bh_by_ph])\n            target[pos[0], pos[1],assigned_anchor_box, 5+int(label)] = 1\n            \n            to_exclude.append(assigned_anchor_box) \n            \n        return target\n    def inverse_target(self, ground_truth, S=S, SCALE=SCALE, anchor_boxes=ANCHOR_BOXES):\n        device = ground_truth.device\n        cx = cy = torch.tensor([i for i in range(13)], device=device)\n        ground_truth = ground_truth.permute(0, 3, 4, 2, 1)\n        ground_truth[..., 1:2, :, :] += cx\n        ground_truth = ground_truth.permute(0, 1, 2, 4, 3)\n        ground_truth[..., 2:3, :, :] += cy                             \n        ground_truth = ground_truth.permute(0, 3, 4, 1, 2)\n    \n        ground_truth[..., 1:3] *= 32\n        ground_truth[..., 3:5] *= torch.tensor(anchor_boxes, device=device)  \n        ground_truth[..., 3:5] *= 32 \n    \n        bbox = ground_truth[ground_truth[..., 0] == 1][..., 1:5]\n        if ground_truth[ground_truth[..., 0] == 1][..., 5:].numel() % 4 == 0:\n            _, labels = torch.max(ground_truth[ground_truth[..., 0] == 1][..., 5:].view(-1, 4), dim=-1)\n        else:\n            _, labels = torch.max(ground_truth[ground_truth[..., 0] == 1][..., 5:], dim=-1)\n    \n        return bbox, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:33.031931Z","iopub.execute_input":"2024-12-12T15:33:33.032446Z","iopub.status.idle":"2024-12-12T15:33:34.352971Z","shell.execute_reply.started":"2024-12-12T15:33:33.032418Z","shell.execute_reply":"2024-12-12T15:33:34.352026Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"transforms = v2.Compose([\n    v2.RandomResizedCrop(size=(416, 416), scale=(0.9,1), antialias=True),\n    v2.RandomPhotometricDistort(p=0.2),\n    v2.RandomHorizontalFlip(p=0.2),\n    v2.RandomZoomOut(p=0.2, side_range=(1.0,1.5), fill={tv_tensors.Image: (0, 100, 0), \"others\": 0}),\n    v2.RandomIoUCrop(min_scale = 0.9, max_scale = 1, max_aspect_ratio=1.25, min_aspect_ratio=0.75),\n    v2.Resize((416,416), antialias=True),\n    v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    v2.SanitizeBoundingBoxes(),\n])\n\ntests_transforms = v2.Compose([\n            v2.ToDtype(torch.float32, scale=True),  \n            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),]\n            )\n\n\nrev_transform = v2.Compose([\n     v2.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n    v2.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.354066Z","iopub.execute_input":"2024-12-12T15:33:34.354405Z","iopub.status.idle":"2024-12-12T15:33:34.364601Z","shell.execute_reply.started":"2024-12-12T15:33:34.354376Z","shell.execute_reply":"2024-12-12T15:33:34.363941Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"## Intatntiate dataset \ndata = GDITAerialDataset('/kaggle/input/gdit-0-1', transform = transforms)\ndevice = DEVICE\nbatch_size = BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.365528Z","iopub.execute_input":"2024-12-12T15:33:34.365780Z","iopub.status.idle":"2024-12-12T15:33:34.382611Z","shell.execute_reply.started":"2024-12-12T15:33:34.365750Z","shell.execute_reply":"2024-12-12T15:33:34.381881Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_data, test_data = torch.utils.data.random_split(data, [0.9, 0.1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.383520Z","iopub.execute_input":"2024-12-12T15:33:34.383773Z","iopub.status.idle":"2024-12-12T15:33:34.388168Z","shell.execute_reply.started":"2024-12-12T15:33:34.383753Z","shell.execute_reply":"2024-12-12T15:33:34.387440Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n        train_data,\n        batch_size=batch_size, \n        shuffle = True,\n        num_workers=0,  \n        pin_memory=True,\n        drop_last=False,\n    )\n\nval_loader = torch.utils.data.DataLoader(\n        test_data,\n        batch_size=batch_size, \n        shuffle = False,\n        num_workers=0,  \n        pin_memory=True,\n        drop_last=False,\n    )\n\nprint('The ImageNet train set is ready. Size : {}'.format(len(train_loader)*batch_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.389128Z","iopub.execute_input":"2024-12-12T15:33:34.389593Z","iopub.status.idle":"2024-12-12T15:33:34.400420Z","shell.execute_reply.started":"2024-12-12T15:33:34.389569Z","shell.execute_reply":"2024-12-12T15:33:34.399604Z"}},"outputs":[{"name":"stdout","text":"The ImageNet train set is ready. Size : 704\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"dataloaders = {}\n\ndataloaders['train'] = train_loader\ndataloaders['val'] = val_loader\n\ndataset_sizes = {'train': len(train_loader)*batch_size,\n                'val': len(val_loader)*batch_size}\nprint(dataset_sizes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.401377Z","iopub.execute_input":"2024-12-12T15:33:34.401606Z","iopub.status.idle":"2024-12-12T15:33:34.411622Z","shell.execute_reply.started":"2024-12-12T15:33:34.401588Z","shell.execute_reply":"2024-12-12T15:33:34.410890Z"}},"outputs":[{"name":"stdout","text":"{'train': 704, 'val': 96}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\n# tuple -> (out_channels, kernel_size)\n# M -> MaxPool, stage1 and stage2 are backbone\nDARKNET_BACKBONE = {\n    \"stage1_conv\": [\n        (32, 3),\n        \"M\",\n        (64, 3),\n        \"M\",\n        (128, 3),\n        (64, 1),\n        (128, 1),\n        \"M\",\n        (256, 3),\n        (128, 1),\n        (256, 3),\n        \"M\",\n        (512, 3),\n        (256, 1),\n        (512, 3),\n        (256, 1),\n        (512, 3),\n    ],\n    \"stage2_conv\": [\"M\", (1024, 3), (512, 1), (1024, 3), (512, 3), (1024, 3)],\n    \"fcn_layer_in_channel\": 3072,\n    \"fcn_layers\": [(1024, 3), (1024, 3), (1024, 3)],\n}\n\n\ndef make_conv_layers(arch_config, in_channels=3):\n    layers = []\n    in_channels = in_channels\n    for value in arch_config:\n        if type(value) == tuple:\n            out_channels, kernel_size = value\n            layers += [\n                nn.Conv2d(\n                    in_channels, out_channels, kernel_size, padding=\"same\", bias=False\n                ),  # not using bias as batchnorm\n                nn.BatchNorm2d(value[0]),\n                nn.LeakyReLU(negative_slope=0.1),\n            ]\n\n            in_channels = out_channels\n\n        elif value == \"M\":\n            layers += [nn.MaxPool2d(kernel_size=2)]\n\n    return nn.Sequential(*layers)\n\nDARKNET_BACKBONE = {\n    \"stage1_conv\": [\n        (32, 3),\n        \"M\",\n        (64, 3),\n        \"M\",\n        (128, 3),\n        (64, 1),\n        (128, 1),\n        \"M\",\n        (256, 3),\n        (128, 1),\n        (256, 3),\n        \"M\",\n        (512, 3),\n        (256, 1),\n        (512, 3),\n        (256, 1),\n        (512, 3),\n    ],\n    \"stage2_conv\": [\"M\", (1024, 3), (512, 1), (1024, 3), (512, 3), (1024, 3)],\n    \"fcn_layer_in_channel\": 3072,\n    \"fcn_layers\": [(1024, 3), (1024, 3), (1024, 3)],\n}\n\n\n\nclass YOLOv2(nn.Module):\n    def __init__(\n        self, backbone_config=DARKNET_BACKBONE, no_of_classes=C, no_of_anchor_box=N\n    ):\n        super().__init__()\n        self.in_channels = 3\n        self.arch_config = backbone_config\n        self.no_of_anchor_box = no_of_anchor_box\n        self.no_of_classes = no_of_classes\n        self.output_layer_in_channels = self.arch_config[\"fcn_layers\"][-1][0]\n\n        # no of anchor boxes * (4 bb + 4 class prob + object confidence score)*13*13 for dataset with 4 classes\n        self.output_channels = self.no_of_anchor_box * (self.no_of_classes + 1 + 4)\n\n        # Conv Layers\n        self.stage1_conv_layers = make_conv_layers(self.arch_config[\"stage1_conv\"])\n        self.stage2_conv_layers = make_conv_layers(\n            self.arch_config[\"stage2_conv\"],\n            in_channels=self.arch_config[\"stage1_conv\"][-1][0],\n        )\n        self.fcn_layers = make_conv_layers(\n            self.arch_config[\"fcn_layers\"],\n            in_channels=self.arch_config[\"fcn_layer_in_channel\"],\n        )\n\n        self.ouput_layer = nn.Conv2d(\n            in_channels=self.output_layer_in_channels,\n            out_channels=self.output_channels,\n            kernel_size=1,\n            padding=\"same\",\n        )\n\n    def forward(self, x):\n        x1 = self.stage1_conv_layers(x)\n        x2 = self.stage2_conv_layers(x1)\n        _, _, height, width = x1.size()\n\n        part1 = x1[:, :, : height // 2, : width // 2]\n        part2 = x1[:, :, : height // 2, width // 2 :]\n        part3 = x1[:, :, height // 2 :, : width // 2]\n        part4 = x1[:, :, height // 2 :, width // 2 :]\n        residual = torch.cat((part1, part2, part3, part4), dim=1)\n        x_concat = torch.cat((x2, residual), dim=1)\n        x3 = self.fcn_layers(x_concat)\n        out = self.ouput_layer(x3)\n        new_out = out.permute(0,2,3,1).contiguous()\n        \n        return new_out.view(new_out.size(0),new_out.size(1),new_out.size(2), self.no_of_anchor_box, 5+self.no_of_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.415050Z","iopub.execute_input":"2024-12-12T15:33:34.415291Z","iopub.status.idle":"2024-12-12T15:33:34.429601Z","shell.execute_reply.started":"2024-12-12T15:33:34.415271Z","shell.execute_reply":"2024-12-12T15:33:34.428746Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n\nclass YoloV2_Loss(torch.nn.Module):\n    def __init__(self, C=1 ):\n        super().__init__()\n        self.lambda_no_obj = torch.tensor(1.3, device=DEVICE)\n        self.lambda_obj = torch.tensor(1.0, device=DEVICE)\n#         self.lambda_prior = torch.tensor(1.0, device=DEVICE)\n        self.lambda_class = torch.tensor(1.0, device=DEVICE)\n        self.lambda_bb_cord = torch.tensor(5.0, device=DEVICE)\n        self.C = C\n        self.binary_loss = (\n            BCEWithLogitsLoss()\n        ) \n        self.logistic_loss = CrossEntropyLoss() \n        self.regression_loss = MSELoss()\n\n    def forward(self, pred, ground_truth):\n        obj = ground_truth[..., 0] == 1\n        no_obj = ground_truth[..., 0] == 0\n        no_obj_loss = self.binary_loss(\n            pred[no_obj][[..., 0]], ground_truth[no_obj][[..., 0]]\n        )\n        obj_loss = self.binary_loss(pred[obj][[..., 0]], ground_truth[obj][[..., 0]])\n        pred_bb = torch.cat(\n            (torch.sigmoid(pred[obj][..., 1:3]), torch.exp(pred[obj][..., 3:5])), dim=-1  # B*S*S*A, 4\n        )  \n        gt_bb = ground_truth[obj][..., 1:5]\n        bb_cord_loss = self.regression_loss(pred_bb, gt_bb)\n        pred_prob = pred[obj][..., 5:] \n    \n        class_loss = self.logistic_loss(\n            pred_prob, ground_truth[obj][..., 5:]\n        )\n    \n        total_loss = (\n            self.lambda_bb_cord * bb_cord_loss\n            + self.lambda_no_obj * no_obj_loss\n            + self.lambda_obj * obj_loss\n            + self.lambda_class * class_loss\n        )\n        \n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.430576Z","iopub.execute_input":"2024-12-12T15:33:34.430836Z","iopub.status.idle":"2024-12-12T15:33:34.444850Z","shell.execute_reply.started":"2024-12-12T15:33:34.430815Z","shell.execute_reply":"2024-12-12T15:33:34.444171Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"g = torch.Generator().manual_seed(0)\n\npred = torch.randn((32, 13,13,5,6),generator=g)\nground_truth = torch.randn((32, 13,13,5,6),generator=g)\nground_truth[..., 0] = torch.empty_like(ground_truth[..., 0], dtype=torch.long).random_(2)\nground_truth[...,5:] = torch.empty_like(ground_truth[..., 5:], dtype=torch.long).random_(2)\nloss = YoloV2_Loss()\nprint(loss(pred, ground_truth))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.445745Z","iopub.execute_input":"2024-12-12T15:33:34.446005Z","iopub.status.idle":"2024-12-12T15:33:34.711048Z","shell.execute_reply.started":"2024-12-12T15:33:34.445986Z","shell.execute_reply":"2024-12-12T15:33:34.710117Z"}},"outputs":[{"name":"stdout","text":"tensor(25.8758, device='cuda:0')\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model = YOLOv2()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:34.712280Z","iopub.execute_input":"2024-12-12T15:33:34.712616Z","iopub.status.idle":"2024-12-12T15:33:35.344526Z","shell.execute_reply.started":"2024-12-12T15:33:34.712587Z","shell.execute_reply":"2024-12-12T15:33:35.343829Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"img, target = data[0]\ntarget = target.unsqueeze(dim=0)\npred = model(img.unsqueeze_(dim=0))\nloss(pred, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:35.345612Z","iopub.execute_input":"2024-12-12T15:33:35.345951Z","iopub.status.idle":"2024-12-12T15:33:35.916875Z","shell.execute_reply.started":"2024-12-12T15:33:35.345921Z","shell.execute_reply":"2024-12-12T15:33:35.915982Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor(2.4562, device='cuda:0', grad_fn=<AddBackward0>)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"darknet19_wts = torch.load('/kaggle/input/darknet-19/pytorch/default/1/darknet_19_state.pt')\nyolo_state = model.state_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:35.918122Z","iopub.execute_input":"2024-12-12T15:33:35.918519Z","iopub.status.idle":"2024-12-12T15:33:36.000982Z","shell.execute_reply.started":"2024-12-12T15:33:35.918486Z","shell.execute_reply":"2024-12-12T15:33:36.000291Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"match_keys =[] \n\nfor key, value in yolo_state.items():\n    if key.startswith('stage'):\n        match_keys.append(key)\n\nprint('Total Layers Matched:', len(match_keys)/6)\n\nprint('To Verify, Before:', yolo_state[match_keys[0]].sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:36.002171Z","iopub.execute_input":"2024-12-12T15:33:36.002510Z","iopub.status.idle":"2024-12-12T15:33:36.009185Z","shell.execute_reply.started":"2024-12-12T15:33:36.002480Z","shell.execute_reply":"2024-12-12T15:33:36.008264Z"}},"outputs":[{"name":"stdout","text":"Total Layers Matched: 18.0\nTo Verify, Before: tensor(2.8270)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"with torch.no_grad():\n    for des_key, src_key in zip(match_keys, darknet19_wts.keys()):\n        if yolo_state[des_key].shape == darknet19_wts[src_key].shape:\n            yolo_state[des_key] = darknet19_wts[src_key]\n        else:\n            print('Weight Transfer Failed')\n            break\n        \nprint('To Verify, After:', yolo_state[match_keys[0]].sum())\nmodel.load_state_dict(yolo_state)\n\nprint('Weight transfer compelte')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:36.010592Z","iopub.execute_input":"2024-12-12T15:33:36.010929Z","iopub.status.idle":"2024-12-12T15:33:36.031695Z","shell.execute_reply.started":"2024-12-12T15:33:36.010897Z","shell.execute_reply":"2024-12-12T15:33:36.030901Z"}},"outputs":[{"name":"stdout","text":"To Verify, After: tensor(-13.6342)\nWeight transfer compelte\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install torcheval\nfrom torcheval.metrics import AUC\ndef mean_average_precision(predictions, targets, iou_thres_nms = 0.4, iou_thres_for_corr_predn =0.4, C=C,):\n    ep = 1e-6\n    processed_preds = process_preds(predictions).clone()\n    pr_matrix = torch.empty(9,C, 2) \n\n    for thres in range(1, 10, 1):\n        \n        ground_truth = targets.clone()\n        \n        conf_thres = thres/10\n        \n        local_pr_matrix = torch.zeros(C, 3)  \n        \n        \n        for i in range(processed_preds.size(0)):\n            preds = processed_preds[i]\n            obj = preds[...,0] > conf_thres\n\n            bboxes = torch.flatten(preds[obj][...,1:5], end_dim = -2)\n            scores = torch.flatten(preds[obj][...,0])\n            _, ind = torch.max(preds[obj][..., 5:],dim = -1)\n            classes = torch.flatten(ind)\n\n\n            best_boxes = non_max_suppression(bboxes, scores, iou_thres_nms)\n\n\n\n            filtered_bbox = bboxes[best_boxes]\n            filtered_classes = classes[best_boxes]\n            gt_bboxes, labels = data.inverse_target(ground_truth[i].unsqueeze(0)) \n            tracker = torch.zeros_like(labels)   # to keep track of matched boxes\n             \n            for c in range(C):\n                total_preds = torch.sum(filtered_classes==c)\n                corr_preds = 0\n                actual_count = torch.sum(labels==c)\n                for box in filtered_bbox[filtered_classes==c]:\n                    best_iou = 0\n                    for index, value in enumerate(labels) :\n                        if c == value:\n\n                            iou = intersection_over_union(box, gt_bboxes[index])  #format is cx,cy, w,h\n\n                            if iou > best_iou and tracker[index]==0:\n                                best_iou = iou\n                                temp = index\n                            \n                    if best_iou > iou_thres_for_corr_predn:\n                        tracker[temp] = 1\n                        corr_preds+=1\n\n\n                local_pr_matrix[c] += torch.tensor([corr_preds, total_preds, actual_count])\n            \n            precision , recall = local_pr_matrix[:,0]/(local_pr_matrix[:,1]+ep), local_pr_matrix[:,0]/(local_pr_matrix[:,2]+ep) \n\n            pr_matrix[thres-1] = torch.cat((precision.view(-1,1), recall.view(-1,1)), dim=1) \n        \n    pr_matrix = pr_matrix.permute(1,0,2) \n\n    metric = AUC(n_tasks=C)\n    metric.update(pr_matrix[...,0], pr_matrix[...,1])\n    average_precision = metric.compute()\n    average_precision = metric.compute()\n\n    return average_precision.mean()          \n    for thres in range(1, 10, 1):\n        \n        ground_truth = targets.clone()\n        \n        conf_thres = thres/10\n        \n        local_pr_matrix = torch.zeros(C, 3) \n        \n        \n        for i in range(processed_preds.size(0)): \n            preds = processed_preds[i]\n            obj = preds[...,0] > conf_thres\n\n            bboxes = torch.flatten(preds[obj][...,1:5], end_dim = -2)\n            scores = torch.flatten(preds[obj][...,0])\n            _, ind = torch.max(preds[obj][..., 5:],dim = -1)\n            classes = torch.flatten(ind)\n\n            best_boxes = non_max_suppression(bboxes, scores, iou_thres_nms)\n\n            filtered_bbox = bboxes[best_boxes]\n            filtered_classes = classes[best_boxes]\n\n            gt_bboxes, labels = data.inverse_target(ground_truth[i].unsqueeze(0))\n            tracker = torch.zeros_like(labels)\n             \n            for c in range(C):\n                total_preds = torch.sum(filtered_classes==c)\n                corr_preds = 0\n                actual_count = torch.sum(labels==c)\n                for box in filtered_bbox[filtered_classes==c]:\n                    best_iou = 0\n                    for index, value in enumerate(labels) :\n                        if c == value:\n\n                            iou = intersection_over_union(box, gt_bboxes[index])  #format is cx,cy, w,h\n\n                            if iou > best_iou and tracker[index]==0:\n                                best_iou = iou\n                                temp = index\n                            \n                    if best_iou > iou_thres_for_corr_predn:\n                        tracker[temp] = 1\n                        corr_preds+=1\n\n\n                local_pr_matrix[c] += torch.tensor([corr_preds, total_preds, actual_count])\n\n            \n            pr_matrix[thres-1] = torch.cat((precision.view(-1,1), recall.view(-1,1)), dim=1) \n\n    pr_matrix = pr_matrix.permute(1,0,2)  \n    metric = AUC(n_tasks=C)\n    metric.update(pr_matrix[...,0], pr_matrix[...,1])\n    average_precision = metric.compute()\n    average_precision = metric.compute()\n\n    return average_precision.mean()          ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:36.032857Z","iopub.execute_input":"2024-12-12T15:33:36.033248Z","iopub.status.idle":"2024-12-12T15:33:44.874490Z","shell.execute_reply.started":"2024-12-12T15:33:36.033220Z","shell.execute_reply":"2024-12-12T15:33:44.873593Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torcheval in /opt/conda/lib/python3.10/site-packages (0.0.7)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.9.0)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"pred = model(img)\nloss(pred, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:44.875664Z","iopub.execute_input":"2024-12-12T15:33:44.875951Z","iopub.status.idle":"2024-12-12T15:33:45.331166Z","shell.execute_reply.started":"2024-12-12T15:33:44.875925Z","shell.execute_reply":"2024-12-12T15:33:45.330217Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor(3.6153, device='cuda:0', grad_fn=<AddBackward0>)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"def check_model_accuracy(preds, targets ,thres = 0.5 ):\n    total_class, class_corr = 0,0\n    total_obj, obj_corr = 0,0\n    total_no_obj, no_obj_corr = 0, 0\n    sig = torch.nn.Sigmoid()\n    \n    obj = targets[..., 0] == 1 \n    no_obj = targets[..., 0] == 0 \n    \n    preds[..., 0] = sig(preds[..., 0])\n    \n    \n    class_corr = torch.sum((torch.argmax(preds[obj][..., 5:], dim=-1) == torch.argmax(targets[obj][...,5:],dim=-1)))\n\n    total_class = torch.sum(obj)\n    obj_corr = torch.sum(preds[obj][..., 0]>thres)\n    total_obj = torch.sum(obj)+ 1e-6  \n    \n    no_obj_corr = torch.sum(preds[no_obj][..., 0]<thres)\n   \n    total_no_obj = torch.sum(no_obj)\n    \n  \n    return torch.tensor([total_class, class_corr, total_obj, obj_corr, total_no_obj, no_obj_corr])\n\ndef cal_epoch_acc(total_class_pred, class_corr, total_obj_prd, obj_corr, total_no_obj, no_obj_corr):\n    print('Class Score (R)', 100*class_corr/total_class_pred)\n    print('Object Score (R)', 100*obj_corr/total_obj_prd)\n    print('No object Score (R)', 100*no_obj_corr/total_no_obj)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:45.332281Z","iopub.execute_input":"2024-12-12T15:33:45.332532Z","iopub.status.idle":"2024-12-12T15:33:45.339271Z","shell.execute_reply.started":"2024-12-12T15:33:45.332513Z","shell.execute_reply":"2024-12-12T15:33:45.338593Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def process_preds(preds, anchor_boxes= ANCHOR_BOXES, device = DEVICE):\n    sig = torch.nn.Sigmoid()\n    preds[..., 0:1] = sig(preds[..., 0:1] ) \n    preds[..., 1:3] = sig(preds[..., 1:3]) \n    \n    cx = cy = torch.tensor([i for i in range(S)], device=device)\n    preds = preds.permute((0,3,4, 2,1)) \n    \n    preds[...,1:2,:,:] += cx\n    preds = preds.permute(0,1,2,4,3)\n    preds[...,2:3,:,:] += cy                             \n    preds = preds.permute((0,3,4, 1,2)) \n    \n    preds[..., 1:3]*=32 \n    preds[..., 3:5] = torch.exp(preds[..., 3:5])  \n    preds[...,3:5]*=torch.tensor(anchor_boxes, device=device)  \n    preds[..., 3:5] = preds[..., 3:5]*32  # back to pixel values\n    \n    \n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:45.340569Z","iopub.execute_input":"2024-12-12T15:33:45.341315Z","iopub.status.idle":"2024-12-12T15:33:45.354424Z","shell.execute_reply.started":"2024-12-12T15:33:45.341283Z","shell.execute_reply":"2024-12-12T15:33:45.353638Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torchvision\n\n\ndef non_max_suppression(boxes, scores, io_threshold = 0.4):\n    boxes = convert_to_corners(boxes)\n    keep = torchvision.ops.nms(boxes, scores, io_threshold)\n\n    return keep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:45.355510Z","iopub.execute_input":"2024-12-12T15:33:45.355830Z","iopub.status.idle":"2024-12-12T15:33:45.365401Z","shell.execute_reply.started":"2024-12-12T15:33:45.355803Z","shell.execute_reply":"2024-12-12T15:33:45.364552Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import time\nfrom tqdm.auto import tqdm\nimport shutil\nimport math\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n    since = time.time()\n   \n    best_map = 0\n    \n    tempdir = '/kaggle/working/temp'\n    os.makedirs(tempdir, exist_ok=True)\n    best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n    one_batch_data = {'train': next(iter(dataloaders['train'])),\n                     'val':next(iter(dataloaders['val']))\n                     }\n    \n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        store_preds = 0   #this w\n        \n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                ##For mAP Calculations\n                no_of_batches = math.ceil(dataset_sizes[phase]/batch_size)\n                all_preds = torch.zeros((no_of_batches, batch_size, S, S, N, C+5))\n                all_targets = torch.zeros((no_of_batches, batch_size, S, S, N, C+5))\n            \n            \n            running_loss = 0.0\n            running_acc = torch.zeros(6)\n            \n            i = 0\n            inputs, targets = one_batch_data[phase]\n            for inputs, targets in tqdm(dataloaders[phase], leave=False):\n                inputs = inputs.to(DEVICE)\n                targets = targets.to(DEVICE)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                   \n                    else:\n                        #for mAP\n                        try:\n                            all_preds[i] = outputs.detach().to(DEVICE)\n                            all_targets[i] = targets.detach().to(DEVICE)\n\n                        except:\n                            pass\n                  \n                        i+=1\n                        \n\n                running_loss += loss.item() * inputs.size(0)\n\n                running_acc = running_acc + check_model_accuracy(outputs.detach(), targets.detach() )\n                \n                \n\n            if phase == 'train':\n                scheduler.step()\n                \n        \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_acc / dataset_sizes[phase]\n            \n            print(f'{phase} Loss: {epoch_loss:.4f}')\n            if phase =='val':\n                all_preds = all_preds.view(-1, 13,13,5,6)\n                all_targets = all_targets.view((-1, 13,13,5,6))\n                mAP = mean_average_precision(all_preds.to(\"cuda\"), all_targets.to(\"cuda\") )\n                print('Mean Average Precision : ' , mAP.item())    \n            \n                if epoch % 1 == 0:\n                    cal_epoch_acc(*(running_acc.tolist()))\n\n                # Also saving model associated with best val loss\n                if mAP>best_map:\n                    best_map = mAP\n                    torch.save(model.state_dict(), best_model_params_path)\n                \n        print()\n\n    time_elapsed = time.time() - since\n    print('Mode with Best mAP: ', best_map)\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    \n    model.load_state_dict(torch.load(best_model_params_path))\n\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:45.366393Z","iopub.execute_input":"2024-12-12T15:33:45.366615Z","iopub.status.idle":"2024-12-12T15:33:45.379434Z","shell.execute_reply.started":"2024-12-12T15:33:45.366597Z","shell.execute_reply":"2024-12-12T15:33:45.378660Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(f'Training \\n')\n\ntorch.cuda.empty_cache()\n\n# model = torch.load('/kaggle/working/yolo_100_epo.pth')\n\nmodel = model.to(device)\ncriterion = YoloV2_Loss()\n\noptimizer_ft = torch.optim.Adam(model.parameters(), lr=0.00005)\n\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:45.380489Z","iopub.execute_input":"2024-12-12T15:33:45.380966Z","iopub.status.idle":"2024-12-12T15:33:45.472376Z","shell.execute_reply.started":"2024-12-12T15:33:45.380945Z","shell.execute_reply":"2024-12-12T15:33:45.471638Z"}},"outputs":[{"name":"stdout","text":"Training \n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"#model = model.to(device)\n#model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n#                   num_epochs=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:45.473329Z","iopub.execute_input":"2024-12-12T15:33:45.473627Z","iopub.status.idle":"2024-12-12T15:33:45.477392Z","shell.execute_reply.started":"2024-12-12T15:33:45.473599Z","shell.execute_reply":"2024-12-12T15:33:45.476586Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"torch.save(model, 'YoloModel.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:45.478460Z","iopub.execute_input":"2024-12-12T15:33:45.478742Z","iopub.status.idle":"2024-12-12T15:33:46.316171Z","shell.execute_reply.started":"2024-12-12T15:33:45.478696Z","shell.execute_reply":"2024-12-12T15:33:46.315088Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch\nimport math\n\n# Create an instance of your model\nmodel = YOLOv2()\n\n# Load the state dictionary\nstate_dict = torch.load('/kaggle/input/bestparams/pytorch/default/1/best_model_params.pt')\nmodel.load_state_dict(state_dict)\n\n# Save the model\ntorch.save(model, 'YoloModel.pth')\n\n# Move the model to the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()\n\n# Define phase and calculate number of batches\nphase = 'val'\nno_of_batches = math.ceil(dataset_sizes[phase] / batch_size)\n\n# Initialize all_preds and all_targets\nall_preds = torch.empty((no_of_batches, batch_size, S, S, N, C+5), device=device)\nall_targets = torch.empty((no_of_batches, batch_size, S, S, N, C+5), device=device)\n\n# Iterate through the dataloader\nfor i, (imge, target) in enumerate(dataloaders[phase]):\n    imge = imge.to(device)\n    preds = model(imge)\n    \n    # Handle the last batch size\n    if preds.size(0) != batch_size:\n        all_preds = all_preds[:i * batch_size]\n        all_targets = all_targets[:i * batch_size]\n        break\n    \n    try:\n        all_preds[i] = preds.detach()\n        all_targets[i] = target.detach().to(device)\n    except Exception as e:\n        print(f'Error in batch {i}: {e}')\n        print('Last batch has shape', preds.shape)\n\n# Reshape all_preds and all_targets\nall_preds = all_preds.view(-1, 13, 13, 5, 6)\nall_targets = all_targets.view(-1, 13, 13, 5, 6)\n\nprint(all_preds.shape, all_targets.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:46.317535Z","iopub.execute_input":"2024-12-12T15:33:46.317894Z","iopub.status.idle":"2024-12-12T15:33:53.446807Z","shell.execute_reply.started":"2024-12-12T15:33:46.317864Z","shell.execute_reply":"2024-12-12T15:33:53.445866Z"}},"outputs":[{"name":"stdout","text":"torch.Size([96, 13, 13, 5, 6]) torch.Size([96, 13, 13, 5, 6])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"mean_average_precision(all_preds.to(device), all_targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:33:53.946849Z","iopub.status.idle":"2024-12-12T15:33:53.947292Z","shell.execute_reply.started":"2024-12-12T15:33:53.947076Z","shell.execute_reply":"2024-12-12T15:33:53.947094Z"}},"outputs":[],"execution_count":null}]}