{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10089409,"sourceType":"datasetVersion","datasetId":6221196}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torchvision.transforms as T\n\nimport torch\nimport random\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom torchvision.transforms.functional import to_tensor\nfrom torchvision.io import read_image\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\n\nfrom torch import nn as nn\nfrom torch.nn import functional as Fn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision.models import resnet18\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\n\nimport json\nfrom PIL import ImageDraw, ImageFont\n\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:27.545653Z","iopub.execute_input":"2024-12-12T12:02:27.546493Z","iopub.status.idle":"2024-12-12T12:02:33.278201Z","shell.execute_reply.started":"2024-12-12T12:02:27.546458Z","shell.execute_reply":"2024-12-12T12:02:33.277280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_width, img_height = 576, 576\n\nS = 9       # Divide each image into a SxS grid\nB = 2       # Number of bounding boxes to predict","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:33.279754Z","iopub.execute_input":"2024-12-12T12:02:33.280029Z","iopub.status.idle":"2024-12-12T12:02:33.284115Z","shell.execute_reply.started":"2024-12-12T12:02:33.280004Z","shell.execute_reply":"2024-12-12T12:02:33.283252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize(data):\n    data = TF.normalize(data, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    return data\n\ndef augment(data):\n    if isinstance(data, torch.Tensor):\n        # Convert to PIL image for augmentations\n        data = TF.to_pil_image(data)\n\n    # Get image size for translation bounds (20% of image size)\n    width, height = data.size  # PIL image\n    max_translation = 0.2  # 20% of the image size\n    x_shift = max_translation * width * (2 * random.random() - 1)  # Random shift between -20% to 20%\n    y_shift = max_translation * height * (2 * random.random() - 1)  # Random shift between -20% to 20%\n\n    # Random scaling between 80% to 120%\n    scale = 1.0 + 0.2 * (2 * random.random() - 1)  # Random scale between 0.8 to 1.2\n\n    # Perform affine transformation with random translation and scaling\n    data = TF.affine(data, angle=0.0, scale=scale, translate=(x_shift, y_shift), shear=0.0)\n    \n    # Random brightness adjustment (exposure)\n    data = TF.adjust_brightness(data, 1.0 + 0.5 * (2 * random.random() - 1))  # Factor between 0.5 and 1.5\n\n    # Random saturation adjustment in HSV space\n    data = TF.adjust_saturation(data, 1.0 + 0.5 * (2 * random.random() - 1))  # Factor between 0.5 and 1.5\n\n    # Convert back to tensor after augmentations\n    data = TF.to_tensor(data)\n    \n    return data\n\n\ndef read_image(img_path):\n    image = Image.open(img_path).convert(\"RGB\")  # Convert to RGB format\n    image = to_tensor(image)  # Convert to tensor\n    return image\n\n\nclass GditDataset(Dataset):\n    def __init__(self, set_type, normalize=None, augment=None, target_transform=None):\n        self.set_type = set_type\n        self.normalize = normalize\n        self.augment = augment\n        self.target_transform = target_transform\n        # Define directories for images and labels based on set type\n        self.images_dir = f\"/kaggle/input/gdit-dataset/Dataset/{set_type}/images\"\n        self.labels_dir = f\"/kaggle/input/gdit-dataset/Dataset/{set_type}/labels\"\n\n        # Get all image files\n        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.jpg')]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx, S=S, B=B):\n        # Get image path and read the image\n        img_path = os.path.join(self.images_dir, self.image_files[idx])\n        image = read_image(img_path)\n        resize_transform = T.Resize((576, 576))\n        image = resize_transform(image)\n        \n        # Get the corresponding label file (same filename as the image but with .txt extension)\n        label_file = os.path.join(self.labels_dir, self.image_files[idx].replace('.jpg', '.txt'))\n    \n        # Initialize the labels tensor with zeros\n        labels = torch.zeros(S, S, B * 5)\n    \n        # Read the label file\n        with open(label_file, 'r') as f:\n            for line in f.readlines():\n                # Parse the label line\n                class_id, cx_norm, cy_norm, w_norm, h_norm = map(float, line.strip().split())\n    \n                # Calculate the grid cell indices (i, j) where the object center falls\n                grid_x = int((cx_norm * img_width) / 64)\n                grid_y = int((cy_norm * img_height) / 64)\n    \n\n    \n                # Add the bounding box data to the grid cell\n                for b in range(B):  # Loop over the bounding boxes (B)\n                    offset = b * 5  # Offset for the B bounding boxes\n                    if labels[grid_y, grid_x, offset + 4] == 0:  # Check if this cell is empty\n                        labels[grid_y, grid_x, offset:offset + 5] = torch.tensor([cx_norm, cy_norm, w_norm, h_norm, 1.0])\n                        break  # Stop after assigning the bounding box\n    \n        # Apply optional transformations\n        if self.augment:\n            image = self.augment(image)\n        if self.normalize:\n            image = self.normalize(image)\n        img_c, img_w, img_h = image.shape\n        if self.target_transform:\n            labels = self.target_transform(labels, img_w, img_h)\n    \n        return image, labels\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:33.285491Z","iopub.execute_input":"2024-12-12T12:02:33.285737Z","iopub.status.idle":"2024-12-12T12:02:33.300645Z","shell.execute_reply.started":"2024-12-12T12:02:33.285713Z","shell.execute_reply":"2024-12-12T12:02:33.299736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bbox_to_coords(bbox):\n    cx, cy, w, h, confidence = bbox[..., 0], bbox[..., 1], bbox[..., 2], bbox[..., 3], bbox[..., 4]\n    x1 = cx - w / 2\n    y1 = cy - h / 2\n    x2 = cx + w / 2\n    y2 = cy + h / 2\n    return torch.stack([x1, y1, x2, y2, confidence], dim=-1)\n\n# Function to plot an image with bounding boxes, grid, and a center dot\ndef visualize_image_with_boxes(image, boxes, S=S, B=1):\n    # Convert the tensor to a PIL image if needed\n    if isinstance(image, torch.Tensor):\n        image = T.ToPILImage()(image)\n    \n    img_width, img_height = image.size  # Get image dimensions\n\n    # Create a plot\n    fig, ax = plt.subplots(1, figsize=(8, 8))\n    ax.imshow(image)\n\n    # Draw the 18x18 grid\n    cell_size = 64\n    for i in range(S + 1):\n        # Vertical lines\n        ax.plot([i * cell_size, i * cell_size], [0, img_height], color=\"blue\", linewidth=1, linestyle=\"--\")\n        # Horizontal lines\n        ax.plot([0, img_width], [i * cell_size, i * cell_size], color=\"blue\", linewidth=1, linestyle=\"--\")\n\n    # Draw bounding boxes and their center dots\n    for i in range(S):\n        for j in range(S):\n            for box in range(B):\n                # Extract bounding box data\n                cx_norm, cy_norm, w_norm, h_norm, confidence = boxes[i, j, box * 5: box * 5 + 5]\n                if confidence > 0:  # Only draw boxes with non-zero confidence\n                    # De-normalize the bounding box coordinates\n                    cx = cx_norm * img_width\n                    cy = cy_norm * img_height\n                    w = w_norm * img_width\n                    h = h_norm * img_height\n\n                    # Convert bbox center format to corner format\n                    x1, y1, x2, y2, confidence = bbox_to_coords(\n                        torch.stack([cx, cy, w, h, confidence], dim=-1)\n                    )\n                    \n                    # Draw the rectangle\n                    rect = patches.Rectangle(\n                        (x1, y1), w, h, linewidth=2, edgecolor='r', facecolor='none'\n                    )\n                    ax.add_patch(rect)\n                    \n                    # Plot the center dot\n                    ax.scatter(cx, cy, color=\"red\", s=5)  # Yellow dot, size 5\n\n    plt.show()\n\n# Example dataset and visualization\ndataset = GditDataset(set_type='train')\n\n# Visualize the first 10 images with their bounding boxes\nfor i in range(5, 6):\n    image, boxes = dataset[i]\n    visualize_image_with_boxes(image, boxes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:33.302337Z","iopub.execute_input":"2024-12-12T12:02:33.302605Z","iopub.status.idle":"2024-12-12T12:02:33.915164Z","shell.execute_reply.started":"2024-12-12T12:02:33.302580Z","shell.execute_reply":"2024-12-12T12:02:33.914373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Reshape(nn.Module):\n    def __init__(self, *args):\n        super().__init__()\n        self.shape = tuple(args)\n\n    def forward(self, x):\n        return torch.reshape(x, (-1, *self.shape))\n\nclass YOLOv1ResNet18(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Each box has 5 values (cx, cy, w, h, confidence)\n        self.depth = 5 * B\n\n        # Load backbone ResNet\n        backbone = resnet18(weights='DEFAULT')\n        # backbone.requires_grad_(False)  # Freeze backbone weights\n        # for param in backbone.parameters():\n        #     assert param.requires_grad == False, \"Backbone weights are not frozen.\"\n        # # Remove the classification layers of ResNet\n        backbone.avgpool = nn.Identity()\n        backbone.fc = nn.Identity()\n        \n        # Add detection layers\n        layers = [\n            backbone,\n            \n            Reshape(512, 18, 18),\n\n            # Detection network layers\n            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            nn.LeakyReLU(negative_slope=0.1),\n\n            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(negative_slope=0.1),\n\n            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n            nn.LeakyReLU(negative_slope=0.1),\n\n            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n            nn.LeakyReLU(negative_slope=0.1),\n\n            nn.Flatten(),\n\n            nn.Linear(S * S * 1024, 4096),\n            nn.Dropout(),\n            nn.LeakyReLU(negative_slope=0.1),\n\n            nn.Linear(4096, S * S * self.depth)\n        ]\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return torch.reshape(\n            self.model(x),\n            (x.size(dim=0), S, S, self.depth)\n        )\n\n# model = YOLOv1ResNet18()\n# image, _ = dataset[0]\n\n# if len(image.shape) == 3:  # (C, H, W)\n#     image = image.unsqueeze(0)  # (1, C, H, W)\n    \n# print(f\"Input image shape: {image.shape}\")  # Should be (1, 3, 576, 576)\n\n# # Pass through the model\n# output = model(image)\n# print(f\"Output shape: {output.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:33.916407Z","iopub.execute_input":"2024-12-12T12:02:33.916740Z","iopub.status.idle":"2024-12-12T12:02:33.929732Z","shell.execute_reply.started":"2024-12-12T12:02:33.916706Z","shell.execute_reply":"2024-12-12T12:02:33.928772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SumSquaredErrorLoss(nn.Module):\n    def __init__(self, S=S, B=1, lambda_coord=5, lambda_noobj=0.5):\n        \"\"\"\n        Custom loss function for YOLO-style detection.\n        Args:\n            S: Number of grid cells (e.g., 18x18 grid).\n            B: Number of bounding boxes per grid cell.\n            lambda_coord: Weight for the localization loss.\n            lambda_noobj: Weight for the no-object confidence loss.\n        \"\"\"\n        super(SumSquaredErrorLoss, self).__init__()\n        self.S = S\n        self.B = B\n        self.lambda_coord = lambda_coord\n        self.lambda_noobj = lambda_noobj\n        self.mse = nn.MSELoss(reduction=\"sum\")  # Use mean-squared error for all terms\n\n    def forward(self, predictions, ground_truth):\n        \"\"\"\n        Compute the loss between predictions and ground truth.\n        Args:\n            predictions: Tensor of shape (N, S, S, B*5), where N is the batch size.\n            ground_truth: Tensor of shape (N, S, S, B*5), where N is the batch size.\n        Returns:\n            Total loss (scalar).\n        \"\"\"\n        # Extract the components\n        pred_boxes = predictions[..., :B * 4].reshape(-1, self.S, self.S, self.B, 4)  # (cx, cy, w, h)\n        pred_confidence = predictions[..., B * 4:].reshape(-1, self.S, self.S, self.B)  # confidence\n\n        true_boxes = ground_truth[..., :B * 4].reshape(-1, self.S, self.S, self.B, 4)  # (cx, cy, w, h)\n        true_confidence = ground_truth[..., B * 4:].reshape(-1, self.S, self.S, self.B)  # confidence\n\n        # Localization Loss (only for cells with objects)\n        object_mask = (true_confidence > 0).unsqueeze(-1).expand_as(pred_boxes)   # Mask for cells containing objects, shape: (N, S, S, B, 1)\n        localization_loss = self.mse(\n            pred_boxes[object_mask],\n            true_boxes[object_mask]\n        )  # Only consider bounding boxes where objects exist\n\n        # Confidence Loss\n        confidence_loss_object = self.mse(\n            pred_confidence[true_confidence > 0],\n            true_confidence[true_confidence > 0]\n        )  # Confidence loss for cells with objects\n\n        confidence_loss_noobject = self.mse(\n            pred_confidence[true_confidence == 0],\n            true_confidence[true_confidence == 0]\n        )  # Confidence loss for cells without objects\n\n        # Total Loss\n        total_loss = (\n            self.lambda_coord * localization_loss  # Weighted localization loss\n            + confidence_loss_object  # Confidence loss for objects\n            + self.lambda_noobj * confidence_loss_noobject  # Weighted no-object loss\n        )\n\n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:33.931039Z","iopub.execute_input":"2024-12-12T12:02:33.931301Z","iopub.status.idle":"2024-12-12T12:02:33.944852Z","shell.execute_reply.started":"2024-12-12T12:02:33.931277Z","shell.execute_reply":"2024-12-12T12:02:33.944111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key='043c2bf5ed74194832136d7ff5c4ed072d5c00e2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:33.945893Z","iopub.execute_input":"2024-12-12T12:02:33.946669Z","iopub.status.idle":"2024-12-12T12:02:34.890687Z","shell.execute_reply.started":"2024-12-12T12:02:33.946641Z","shell.execute_reply":"2024-12-12T12:02:34.889797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 8\nEPOCHS = 100\nLEARNING_RATE = 1E-4\n\nwandb.finish()  # Ensure no lingering runs\nwandb.init(\n    project=\"YOLOv1 on GDIT\", \n    config={\n        \"batch_size\": BATCH_SIZE,\n        \"epochs\": EPOCHS,\n        \"learning_rate\": LEARNING_RATE\n    },\n    reinit=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:34.891758Z","iopub.execute_input":"2024-12-12T12:02:34.892202Z","iopub.status.idle":"2024-12-12T12:02:36.036481Z","shell.execute_reply.started":"2024-12-12T12:02:34.892173Z","shell.execute_reply":"2024-12-12T12:02:36.035657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':      # Prevent recursive subprocess creation\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    torch.autograd.set_detect_anomaly(True)         # Check for nan loss\n\n    model = YOLOv1ResNet18()\n    model.to(device)\n    loss_function = SumSquaredErrorLoss()\n\n    # Adam works better\n    # optimizer = torch.optim.SGD(\n    #     model.parameters(),\n    #     lr=LEARNING_RATE,\n    #     momentum=0.9,\n    #     weight_decay=5E-4\n    # )\n    optimizer = torch.optim.Adam(\n        model.parameters(),\n        lr=LEARNING_RATE\n    )\n\n\n    # Load the dataset\n    train_set = GditDataset('train', normalize=normalize, augment=augment)\n    val_set = GditDataset('valid', normalize=normalize, augment=augment)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=BATCH_SIZE,\n        drop_last=True\n    )\n\n    \n    # Create folders\n    new_directory = 'checkpoints'\n    if not os.path.exists(new_directory):\n        os.makedirs(new_directory)\n    now = datetime.now()\n    time = now.strftime('%M-%H-%d-%m-%Y')\n    save_path = f'/kaggle/working/checkpoints/yolov1-{time}'\n\n    \n    #####################\n    #       Train       #\n    #####################\n    \n    epoch_bar = tqdm(total=EPOCHS, desc=\"Total Progress\")\n    best_val_loss = float(1e5)\n    \n    for epoch in range(EPOCHS):\n        model.train()\n        # Training Loop\n        train_loss = 0\n        for images, labels in train_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n        \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n\n            # max_grad_norm = 1.0  # Maximum allowed norm\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            \n            optimizer.step()\n            \n            train_loss += loss.item()  # Accumulate loss (already normalized per batch)\n        \n        train_loss /= len(train_loader)  # Average over all batches\n        \n        # Validation Loop\n        val_loss = 0\n        model.eval()\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images = images.to(device)\n                labels = labels.to(device)\n        \n                outputs = model(images)\n                loss = loss_function(outputs, labels)\n                val_loss += loss.item()\n        \n        val_loss /= len(val_loader)  # Average over all batches\n        \n        print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss:.10f}, Val Loss: {val_loss:.10f}\")\n\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            checkpoint = { \n                'epoch': epoch,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'loss': val_loss,\n            }\n            torch.save(checkpoint, save_path)\n        \n        epoch_bar.update(1)\n        if wandb.run and not wandb.run._is_finished:\n            wandb.log({'Val_loss': val_loss, 'Train_loss': train_loss})\n    epoch_bar.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:02:36.037430Z","iopub.execute_input":"2024-12-12T12:02:36.037681Z","iopub.status.idle":"2024-12-12T13:43:25.560320Z","shell.execute_reply.started":"2024-12-12T12:02:36.037655Z","shell.execute_reply":"2024-12-12T13:43:25.559090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dang beo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:43:25.563464Z","iopub.execute_input":"2024-12-12T13:43:25.563771Z","iopub.status.idle":"2024-12-12T13:43:25.570036Z","shell.execute_reply.started":"2024-12-12T13:43:25.563741Z","shell.execute_reply":"2024-12-12T13:43:25.569055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle varying numbers of bounding boxes per image.\n    Args:\n        batch: List of tuples (image, labels).\n    Returns:\n        A tuple of batched images and a list of corresponding labels.\n    \"\"\"\n    images = []\n    labels = []\n\n    for image, label in batch:\n        images.append(image)\n        labels.append(label)\n\n    # Stack images into a single tensor\n    images = torch.stack(images, dim=0)\n\n    # Labels remain as a list of tensors\n    return images, labels\n\n\n\nclass GditTestDataset(Dataset):\n    def __init__(self, set_type, normalize=None, augment=None, target_transform=None):\n        self.set_type = set_type\n        self.normalize = normalize\n        self.augment = augment\n        self.target_transform = target_transform\n        # Define directories for images and labels based on set type\n        self.images_dir = f\"/kaggle/input/gdit-dataset/Dataset/{set_type}/images\"\n        self.labels_dir = f\"/kaggle/input/gdit-dataset/Dataset/{set_type}/labels\"\n\n        # Get all image files\n        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.jpg')]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # Get image path and read the image\n        img_path = os.path.join(self.images_dir, self.image_files[idx])\n        image = read_image(img_path)\n\n        # Get the corresponding label file (same filename as the image but with .txt extension)\n        label_file = os.path.join(self.labels_dir, self.image_files[idx].replace('.jpg', '.txt'))\n\n        # Read the label file, ignoring the first number (class ID or unused)\n        with open(label_file, 'r') as f:\n            labels = []\n            for line in f.readlines():\n                # Split each line and ignore the first value (0), then map the rest to float\n                _,  cx_norm, cy_norm, w_norm, h_norm = map(float, line.strip().split())\n                \n                labels.append(torch.tensor([cx_norm, cy_norm, w_norm, h_norm]))\n\n        labels = torch.stack(labels) if labels else torch.empty(0, 4)\n\n        # Apply optional transformations\n        if self.augment:\n            image = self.augment(image)\n        if self.normalize:\n            image = self.normalize(image)\n        img_c, img_w, img_h = image.shape\n        if self.target_transform:\n            labels = self.target_transform(labels, img_w, img_h)\n    \n        return image, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:43:25.571272Z","iopub.execute_input":"2024-12-12T13:43:25.571542Z","iopub.status.idle":"2024-12-12T13:43:25.584187Z","shell.execute_reply.started":"2024-12-12T13:43:25.571513Z","shell.execute_reply":"2024-12-12T13:43:25.583263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_iou(box1, box2):\n    \"\"\"\n    Compute IoU between two bounding boxes.\n    Args:\n        box1: Tensor of shape (4), format [x1, y1, x2, y2].\n        box2: Tensor of shape (4), format [x1, y1, x2, y2].\n    Returns:\n        IoU value (float).\n    \"\"\"\n    # Intersection coordinates\n    x1 = torch.max(box1[0], box2[0])\n    y1 = torch.max(box1[1], box2[1])\n    x2 = torch.min(box1[2], box2[2])\n    y2 = torch.min(box1[3], box2[3])\n\n    # Intersection area\n    inter_area = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n\n    # Union area\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union_area = box1_area + box2_area - inter_area\n\n    return inter_area / union_area if union_area > 0 else 0\n\ndef non_max_suppression(boxes, iou_threshold=0.5):\n    \"\"\"\n    Perform Non-Maximum Suppression (NMS) on bounding boxes.\n    Args:\n        boxes: List of bounding boxes [x1, y1, x2, y2, confidence].\n        iou_threshold: IoU threshold for suppression.\n    Returns:\n        Filtered list of bounding boxes after NMS.\n    \"\"\"\n    if len(boxes) == 0:\n        return []\n\n    # Sort boxes by confidence in descending order\n    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n\n    nms_boxes = []\n    while boxes:\n        # Pick the box with the highest confidence\n        chosen_box = boxes.pop(0)\n        nms_boxes.append(chosen_box)\n\n        # Remove boxes with IoU > threshold\n        boxes = [\n            box for box in boxes\n            if compute_iou(torch.tensor(chosen_box[:4]), torch.tensor(box[:4])) < iou_threshold\n        ]\n\n    return nms_boxes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:43:25.585199Z","iopub.execute_input":"2024-12-12T13:43:25.585519Z","iopub.status.idle":"2024-12-12T13:43:25.596351Z","shell.execute_reply.started":"2024-12-12T13:43:25.585483Z","shell.execute_reply":"2024-12-12T13:43:25.595589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_ap(recalls, precisions):\n    \"\"\"\n    Calculate Average Precision (AP) from recall and precision values.\n    Args:\n        recalls: Tensor of recall values.\n        precisions: Tensor of precision values.\n    Returns:\n        AP value (float).\n    \"\"\"\n    recalls = torch.cat([torch.tensor([0.0]), recalls, torch.tensor([1.0])])\n    precisions = torch.cat([torch.tensor([0.0]), precisions, torch.tensor([0.0])])\n\n    # Ensure precision is non-decreasing\n    for i in range(len(precisions) - 1, 0, -1):\n        precisions[i - 1] = torch.max(precisions[i - 1], precisions[i])\n\n    # Compute AP as area under the curve\n    indices = torch.where(recalls[1:] != recalls[:-1])[0]\n    ap = torch.sum((recalls[indices + 1] - recalls[indices]) * precisions[indices + 1])\n    return ap\n\ndef mean_average_precision(pred_boxes, gt_boxes, iou_threshold=0.5):\n    \"\"\"\n    Calculate mAP for a single class (since GDIT dataset has only one class).\n    Args:\n        pred_boxes: List of predicted boxes [batch_idx, x1, y1, x2, y2, confidence, class_id].\n        gt_boxes: List of ground truth boxes [batch_idx, x1, y1, x2, y2, class_id].\n        iou_threshold: IoU threshold for a prediction to be considered correct.\n    Returns:\n        mAP score (float).\n    \"\"\"\n    # Initialize arrays to track true positives and false positives\n    tp = torch.zeros(len(pred_boxes))\n    fp = torch.zeros(len(pred_boxes))\n\n    # Track ground truth usage\n    gt_used = {}\n\n    # Sort predictions by confidence (highest to lowest)\n    pred_boxes.sort(key=lambda x: x[5], reverse=True)\n\n    # Process each predicted box\n    for i, pred_box in enumerate(pred_boxes):\n        batch_idx = pred_box[0]\n        best_iou = 0\n        best_gt_idx = None\n\n        # Find corresponding ground truth for the current predicted box\n        for j, gt_box in enumerate(gt_boxes):\n            if batch_idx != gt_box[0]:  # Ensure same image\n                continue\n\n            iou = compute_iou(torch.tensor(pred_box[1:5]), torch.tensor(gt_box[1:5]))\n            if iou > best_iou:\n                best_iou = iou\n                best_gt_idx = j\n\n        # Check if IoU is above threshold and ground truth is not used\n        if best_iou > iou_threshold and (batch_idx, best_gt_idx) not in gt_used:\n            # True Positive\n            tp[i] = 1\n            gt_used[(batch_idx, best_gt_idx)] = True\n        else:\n            # False Positive\n            fp[i] = 1\n\n    # Calculate cumulative true positives and false positives\n    tp_cumsum = torch.cumsum(tp, dim=0)\n    fp_cumsum = torch.cumsum(fp, dim=0)\n\n    # Recall and precision calculations\n    recalls = tp_cumsum / len(gt_boxes)\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n\n    # Calculate Average Precision (AP)\n    ap = calculate_ap(recalls, precisions)\n    \n    return ap\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:43:25.597586Z","iopub.execute_input":"2024-12-12T13:43:25.597860Z","iopub.status.idle":"2024-12-12T13:43:25.609556Z","shell.execute_reply.started":"2024-12-12T13:43:25.597832Z","shell.execute_reply":"2024-12-12T13:43:25.608751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, iou_threshold=0.5, nms_threshold=0.5):\n    \"\"\"\n    Evaluate YOLO model on a dataset.\n    Args:\n        model: Trained YOLO model.\n        dataloader: Dataloader for the evaluation dataset.\n        iou_threshold: IoU threshold for mAP calculation.\n        nms_threshold: IoU threshold for NMS.\n    Returns:\n        mAP score (float).\n    \"\"\"\n    model.eval()\n    pred_boxes = []\n    true_boxes = []\n\n    yolo_test_set = GditDataset('test', normalize=normalize, augment=None)\n    yolo_test_loader = DataLoader(\n        yolo_test_set,\n        batch_size=BATCH_SIZE,\n        drop_last=True\n    )\n\n    test_set = GditTestDataset('test', normalize=normalize, augment=None)\n    test_loader = DataLoader(\n        test_set,\n        batch_size=BATCH_SIZE,\n        drop_last=True,\n        collate_fn=custom_collate_fn\n    )\n    \n    with torch.no_grad():\n        for batch_idx, (images, labels) in enumerate(yolo_test_loader):\n            # Get model predictions            \n            images = images.to(device)\n            labels = labels.to(device)\n            predictions = model(images)  # Shape: (N, S, S, B*5)\n            \n            \n            # Process each image in the batch\n            for idx in range(images.shape[0]):\n                # Convert predictions to list of bounding boxes\n                image_pred_boxes = []\n                for i in range(predictions.shape[1]):  # S\n                    for j in range(predictions.shape[2]):  # S\n                        for b in range(predictions.shape[3] // 5):  # B\n                            box = predictions[idx, i, j, b*5:b*5+5].cpu().numpy()\n                            cx, cy, w, h, confidence = box\n                            if confidence > 0.5:  # Confidence threshold\n                                x1 = (cx - w / 2) * images.shape[3]\n                                y1 = (cy - h / 2) * images.shape[2]\n                                x2 = (cx + w / 2) * images.shape[3]\n                                y2 = (cy + h / 2) * images.shape[2]\n                                image_pred_boxes.append([x1, y1, x2, y2, confidence])\n\n                # Apply NMS\n                image_pred_boxes = non_max_suppression(image_pred_boxes, nms_threshold)\n                for box in image_pred_boxes:\n                    pred_boxes.append([batch_idx] + box)\n        for images, ground_truth in test_loader:\n            for idx in range(images.shape[0]):\n                # Add ground-truth boxes\n                for gt_box in ground_truth[idx]:\n                    cx, cy, w, h = gt_box[:4]\n                    x1 = (cx - w / 2) * images.shape[3]\n                    y1 = (cy - h / 2) * images.shape[2]\n                    x2 = (cx + w / 2) * images.shape[3]\n                    y2 = (cy + h / 2) * images.shape[2]\n                    true_boxes.append([batch_idx, x1, y1, x2, y2])\n\n    # Compute mAP\n    return mean_average_precision(pred_boxes, true_boxes, iou_threshold)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:43:25.610660Z","iopub.execute_input":"2024-12-12T13:43:25.611110Z","iopub.status.idle":"2024-12-12T13:43:25.624184Z","shell.execute_reply.started":"2024-12-12T13:43:25.611072Z","shell.execute_reply":"2024-12-12T13:43:25.623356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"map_score = evaluate_model(model)\nprint(f\"mAP Score: {map_score}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:43:25.625079Z","iopub.execute_input":"2024-12-12T13:43:25.625345Z","iopub.status.idle":"2024-12-12T13:44:16.313414Z","shell.execute_reply.started":"2024-12-12T13:43:25.625317Z","shell.execute_reply":"2024-12-12T13:44:16.312502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def plot_test_images(MODEL_DIR):\n#     model.eval()\n#     test_set = GditDataset('test', normalize=normalize, augment=False)\n#     test_loader = DataLoader(test_set, batch_size=8, shuffle=True)\n\n#     model = YOLOv1()\n#     model.eval()\n#     checkpoint=\"\"\n#     model.load_state_dict(torch.load(checkpoint)))\n\n#     with torch.no_grad():\n#         test_loss = 0\n#         for images, labels in test_loader:\n#             images = images.to(device)\n#             labels = labels.to(device)\n            \n#             labels = labels.squeeze(dim=1).long()\n#             outputs = model(images)\n\n#             test_loss += loss_function(outputs, labels).item()\n\n#     print(f'Test_loss : {test_loss / len(test_loader)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:44:16.314487Z","iopub.execute_input":"2024-12-12T13:44:16.314753Z","iopub.status.idle":"2024-12-12T13:44:16.319531Z","shell.execute_reply.started":"2024-12-12T13:44:16.314728Z","shell.execute_reply":"2024-12-12T13:44:16.318403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_test_images(MODEL_DIR='models/yolo_v1/08_19_2022/08_42_58')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:44:16.320679Z","iopub.execute_input":"2024-12-12T13:44:16.320975Z","iopub.status.idle":"2024-12-12T13:44:16.330610Z","shell.execute_reply.started":"2024-12-12T13:44:16.320949Z","shell.execute_reply":"2024-12-12T13:44:16.329925Z"}},"outputs":[],"execution_count":null}]}